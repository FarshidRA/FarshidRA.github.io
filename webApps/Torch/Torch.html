<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Interactive Object Detection 3:4</title>
<style>
  body {
    margin:0;
    display:flex;
    justify-content:center;
    align-items:center;
    height:100vh;
    background:#000;
  }
  #container {
    position:relative;
    width:360px;
    height:480px; /* 3:4 aspect ratio */
    background:#000;
    overflow:hidden;
    display:flex;
    justify-content:center;
    align-items:center;
  }
  video#cam {
    /* video displayed in its natural size */
    display:block;
  }
  canvas#overlay {
    position:absolute;
    top:0;
    left:0;
    touch-action: none; /* allow drag on mobile */
  }
</style>
</head>
<body>
<div id="container">
  <video id="cam" autoplay playsinline></video>
  <canvas id="overlay"></canvas>
</div>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.7.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js"></script>
<script>
(async ()=>{
const video = document.getElementById('cam');
const canvas = document.getElementById('overlay');
const ctx = canvas.getContext('2d');
let model;
let boxes = []; // store detected boxes for dragging

// access camera
try {
  const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' }, audio: false });
  video.srcObject = stream;
  await video.play();
} catch(e){
  alert('Camera access not available.');
  return;
}

// set canvas to video size
video.addEventListener('loadedmetadata', ()=>{
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;

  // center video in container (3:4)
  const container = document.getElementById('container');
  video.style.width = video.videoWidth + 'px';
  video.style.height = video.videoHeight + 'px';
});

// load ML model
model = await cocoSsd.load();
console.log('Model loaded');

// dragging state
let dragging = null;
let offset = {x:0, y:0};

canvas.addEventListener('pointerdown', e => {
  const rect = canvas.getBoundingClientRect();
  const x = (e.clientX - rect.left) * (canvas.width/rect.width);
  const y = (e.clientY - rect.top) * (canvas.height/rect.height);

  for(let i=boxes.length-1;i>=0;i--){
    const b = boxes[i];
    if(x>=b.x && x<=b.x+b.w && y>=b.y && y<=b.y+b.h){
      dragging = b;
      offset.x = x - b.x;
      offset.y = y - b.y;
      return;
    }
  }
});

canvas.addEventListener('pointermove', e => {
  if(dragging){
    const rect = canvas.getBoundingClientRect();
    const x = (e.clientX - rect.left) * (canvas.width/rect.width);
    const y = (e.clientY - rect.top) * (canvas.height/rect.height);
    dragging.x = x - offset.x;
    dragging.y = y - offset.y;
  }
});

canvas.addEventListener('pointerup', e => { dragging = null; });
canvas.addEventListener('pointerleave', e => { dragging = null; });

// detection loop
async function detectLoop(){
  if(video.readyState === 4){
    const predictions = await model.detect(video);
    ctx.clearRect(0,0,canvas.width,canvas.height);

    boxes = predictions.map(p => {
      let [x,y,w,h] = p.bbox;
      return {
        x: x,
        y: y,
        w: w,
        h: h,
        class: p.class,
        score: p.score
      };
    });

    for(const b of boxes){
      let color;
      if(b.score<0.5) color='red';
      else if(b.score<0.8) color='yellow';
      else color='lime';

      ctx.strokeStyle=color;
      ctx.lineWidth=2; // thinner line
      ctx.strokeRect(b.x,b.y,b.w,b.h);

      ctx.fillStyle=color;
      ctx.font='14px sans-serif';
      ctx.fillText(b.class + ' ' + Math.round(b.score*100)+'%', b.x+4, b.y+16);
    }
  }
  requestAnimationFrame(detectLoop);
}

detectLoop();
})();
</script>
</body>
</html>
